{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyVAldR0oFGPB07pHndOWn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darvesh-sd/Copy-of-TPSessions.ipynb/blob/main/TP2_Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA0-K_1TkNP7"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Book Scraper\n",
        "----------------------------------\n",
        "\n",
        "This script gathers book titles and cover images from \"Books to Scrape\".\n",
        "It's designed explicitly for easy use on Google Colab. I've included detailed explanations and interactive visualizations to enhance your experience.\n",
        "\n",
        "Author: SEWOOGOLUM DARVESH SHARMA\n",
        "Date: May 14, 2025\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Installing necessary libraries\n",
        "!pip install requests beautifulsoup4 pandas matplotlib\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "from urllib.parse import urljoin\n",
        "from google.colab import files\n",
        "\n",
        "# Base URL and setup variables\n",
        "BASE_URL = \"https://books.toscrape.com/catalogue/\"\n",
        "PAGES_TO_SCRAPE = 5\n",
        "\n",
        "# Setting headers to imitate a browser (so we're less likely to get blocked)\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
        "                  ' AppleWebKit/537.36 (KHTML, like Gecko)'\n",
        "                  ' Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Function to build URLs for each page\n",
        "def build_page_url(page_number):\n",
        "    \"\"\"Create full URL for each individual page.\"\"\"\n",
        "    return f\"{BASE_URL}page-{page_number}.html\"\n",
        "\n",
        "# Extracting file extension from URLs\n",
        "def get_file_extension(url):\n",
        "    \"\"\"Extract the image file extension from URL.\"\"\"\n",
        "    return url.split('.')[-1] if '.' in url else 'unknown'\n",
        "\n",
        "# Function to scrape a single page\n",
        "def scrape_books_from_page(page_url):\n",
        "    \"\"\"Scrape book titles and cover images from a given URL.\"\"\"\n",
        "    print(f\"Scraping data from: {page_url}\")\n",
        "    books_list = []\n",
        "\n",
        "    # Adding random delay to respect site's server\n",
        "    time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    try:\n",
        "        response = requests.get(page_url, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all book listings on page\n",
        "        articles = soup.select('article.product_pod')\n",
        "        for article in articles:\n",
        "            # Extracting the book title\n",
        "            title = article.h3.a['title']\n",
        "\n",
        "            # Extracting the image URL and converting to absolute URL\n",
        "            img_relative_url = article.find('img')['src']\n",
        "            img_absolute_url = urljoin('https://books.toscrape.com/', img_relative_url)\n",
        "\n",
        "            # Determining file extension\n",
        "            extension = get_file_extension(img_absolute_url)\n",
        "\n",
        "            # Storing data\n",
        "            books_list.append({\n",
        "                'Title': title,\n",
        "                'Image URL': img_absolute_url,\n",
        "                'Extension': extension\n",
        "            })\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Oops! Couldn't retrieve {page_url}. Error: {e}\")\n",
        "\n",
        "    return books_list\n",
        "\n",
        "# Function to visualize book cover image extensions\n",
        "def visualize_extensions(df):\n",
        "    \"\"\"Visualizes frequency of image extensions with a bar chart.\"\"\"\n",
        "    print(\"Visualizing image extension frequency...\")\n",
        "    df['Extension'].value_counts().plot(kind='bar', figsize=(10, 6), color='lightcoral')\n",
        "    plt.title('Book Cover Image Extensions', fontsize=16)\n",
        "    plt.xlabel('Extension', fontsize=14)\n",
        "    plt.ylabel('Count', fontsize=14)\n",
        "    plt.grid(axis='y', linestyle='--')\n",
        "    plt.show()\n",
        "\n",
        "# Function to save and download results\n",
        "def save_and_download(df, filename='books_scraped.csv'):\n",
        "    \"\"\"Save DataFrame to CSV and prompt user to download file.\"\"\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Data saved successfully as '{filename}'! Here's a quick preview:\")\n",
        "    display(df.head(10))\n",
        "    files.download(filename)\n",
        "\n",
        "# Main scraping process\n",
        "print(\"Let's start scraping some books!\")\n",
        "all_books = []\n",
        "\n",
        "for page_num in range(1, PAGES_TO_SCRAPE + 1):\n",
        "    url = build_page_url(page_num)\n",
        "    page_books = scrape_books_from_page(url)\n",
        "    print(f\"Page {page_num} gave us {len(page_books)} books. Nice!\")\n",
        "    all_books.extend(page_books)\n",
        "\n",
        "print(f\"\\nScraping complete! Collected a total of {len(all_books)} books.\")\n",
        "\n",
        "# Creating DataFrame for analysis\n",
        "df_books = pd.DataFrame(all_books)\n",
        "\n",
        "# Visualizing results\n",
        "visualize_extensions(df_books)\n",
        "\n",
        "# Saving and downloading results\n",
        "save_and_download(df_books)\n",
        "\n",
        "# Giving the user some interactive tips\n",
        "print(\"\\n Congrats! You've successfully scraped data from 'Books to Scrape'.\")\n",
        "print(\"You can explore further using these commands:\")\n",
        "print(\"- df_books.head() : See the first few rows of your data\")\n",
        "print(\"- df_books['Title'].value_counts() : See the frequency of book titles\")\n",
        "print(\"- df_books.groupby('Extension').size() : Check counts by image extension\")"
      ]
    }
  ]
}